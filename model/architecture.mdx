---
title: 'GPT Architecture'
description: 'Understanding the GPT model architecture in nanoGPT'
icon: 'sitemap'
---

## Overview

The nanoGPT implementation provides a clean, from-scratch implementation of the GPT (Generative Pre-trained Transformer) architecture. The model is defined entirely in a single file (`model.py`) and follows the architecture introduced in the original GPT-2 paper.

<Info>
This implementation is based on the official GPT-2 TensorFlow implementation by OpenAI and the HuggingFace transformers PyTorch implementation.
</Info>

## Architecture Components

The GPT model consists of several key components arranged in a transformer architecture:

```python
self.transformer = nn.ModuleDict(dict(
    wte = nn.Embedding(config.vocab_size, config.n_embd),
    wpe = nn.Embedding(config.block_size, config.n_embd),
    drop = nn.Dropout(config.dropout),
    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
    ln_f = LayerNorm(config.n_embd, bias=config.bias),
))
self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
```

### Embedding Layers

<Tabs>
  <Tab title="Token Embeddings (wte)">
    Converts token IDs to dense vector representations.
    
    - Input: Token indices `(batch_size, sequence_length)`
    - Output: Token embeddings `(batch_size, sequence_length, n_embd)`
    - Vocabulary size: 50304 (padded to nearest multiple of 64)
  </Tab>
  
  <Tab title="Position Embeddings (wpe)">
    Adds positional information to token embeddings.
    
    - Input: Position indices `(sequence_length,)`
    - Output: Position embeddings `(sequence_length, n_embd)`
    - Maximum sequence length: Defined by `block_size` (default: 1024)
  </Tab>
</Tabs>

### Transformer Blocks

The model stacks multiple transformer blocks (default: 12 layers). Each block contains:

1. **Layer Normalization** - Pre-normalization before attention
2. **Causal Self-Attention** - Multi-head self-attention with causal masking
3. **Layer Normalization** - Pre-normalization before feedforward
4. **MLP (Feedforward Network)** - Position-wise feedforward network
5. **Residual Connections** - Skip connections around attention and MLP

### Language Model Head

The final linear projection layer maps the transformer outputs back to vocabulary logits:

```python
self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
```

<Note>
**Weight Tying**: The token embedding weights are tied with the language model head weights (`self.transformer.wte.weight = self.lm_head.weight`). This reduces the total parameter count and has been shown to improve performance.
</Note>

## Forward Pass

The forward pass through the model follows this sequence:

```python
def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()
    assert t <= self.config.block_size
    pos = torch.arange(0, t, dtype=torch.long, device=device)

    # Forward the GPT model itself
    tok_emb = self.transformer.wte(idx)          # (b, t, n_embd)
    pos_emb = self.transformer.wpe(pos)          # (t, n_embd)
    x = self.transformer.drop(tok_emb + pos_emb) # (b, t, n_embd)
    
    for block in self.transformer.h:
        x = block(x)                             # (b, t, n_embd)
    
    x = self.transformer.ln_f(x)                 # (b, t, n_embd)
    
    if targets is not None:
        # Training: compute loss for all positions
        logits = self.lm_head(x)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                              targets.view(-1), ignore_index=-1)
    else:
        # Inference: only compute logits for last position
        logits = self.lm_head(x[:, [-1], :])
        loss = None
    
    return logits, loss
```

### Step-by-Step Breakdown

<Accordion title="1. Input Processing">
  **Token and Position Embeddings**
  
  The input token indices are converted to embeddings and combined with positional information:
  
  - Token embeddings: `tok_emb = wte(idx)` → shape `(batch, time, n_embd)`
  - Position embeddings: `pos_emb = wpe(pos)` → shape `(time, n_embd)`
  - Combined: `x = tok_emb + pos_emb` with dropout applied
</Accordion>

<Accordion title="2. Transformer Blocks">
  **Sequential Processing Through Layers**
  
  The embeddings pass through `n_layer` transformer blocks sequentially:
  
  ```python
  for block in self.transformer.h:
      x = block(x)
  ```
  
  Each block applies:
  - Pre-norm → Multi-head attention → Residual connection
  - Pre-norm → Feedforward → Residual connection
</Accordion>

<Accordion title="3. Final Layer Norm">
  **Output Normalization**
  
  After all transformer blocks, a final layer normalization is applied:
  
  ```python
  x = self.transformer.ln_f(x)
  ```
</Accordion>

<Accordion title="4. Language Model Head">
  **Generating Logits**
  
  The normalized outputs are projected to vocabulary logits:
  
  - **Training mode**: Compute logits for all positions and calculate cross-entropy loss
  - **Inference mode**: Only compute logits for the last position (optimization)
</Accordion>

## Key Design Decisions

### Pre-Normalization

The implementation uses pre-normalization (LayerNorm before attention/MLP) rather than post-normalization:

```python
def forward(self, x):
    x = x + self.attn(self.ln_1(x))  # Pre-norm before attention
    x = x + self.mlp(self.ln_2(x))   # Pre-norm before MLP
    return x
```

This has been shown to improve training stability, especially for deeper models.

### Causal Masking

The attention mechanism uses causal masking to ensure that predictions for position `i` can only depend on positions `< i`:

<Info>
This autoregressive property is essential for language modeling, where we predict the next token based only on previous tokens.
</Info>

### Flash Attention

When available (PyTorch >= 2.0), the model uses Flash Attention for improved efficiency:

```python
self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
if self.flash:
    y = torch.nn.functional.scaled_dot_product_attention(
        q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True
    )
```

## Model Sizes

The architecture supports various model sizes, matching the GPT-2 variants:

| Model | Layers | Heads | Embedding Dim | Parameters |
|-------|--------|-------|---------------|------------|
| GPT-2 | 12 | 12 | 768 | 124M |
| GPT-2 Medium | 24 | 16 | 1024 | 350M |
| GPT-2 Large | 36 | 20 | 1280 | 774M |
| GPT-2 XL | 48 | 25 | 1600 | 1558M |

## Weight Initialization

The model uses careful weight initialization following the GPT-2 paper:

```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

<Note>
Residual projection layers (`c_proj.weight`) receive special scaled initialization: `std=0.02/math.sqrt(2 * n_layer)` to account for the accumulation of residual connections.
</Note>

## Parameter Counting

The model provides a method to count parameters, with an option to exclude position embeddings:

```python
def get_num_params(self, non_embedding=True):
    n_params = sum(p.numel() for p in self.parameters())
    if non_embedding:
        n_params -= self.transformer.wpe.weight.numel()
    return n_params
```

Position embeddings are typically excluded because they don't scale with context length during inference.
