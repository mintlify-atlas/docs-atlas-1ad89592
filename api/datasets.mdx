---
title: "Datasets"
description: "Supported datasets and how to add your own custom data for training"
---

## Built-in Datasets

NanoGPT includes three prepared datasets for quick experimentation and training. Each dataset demonstrates different tokenization approaches and scales.

### Shakespeare (Character-Level)

**Dataset:** `shakespeare_char`

<Card title="Quick Start" icon="play" href="/quickstart">
  Perfect for beginners - trains a character-level GPT in ~3 minutes on a GPU
</Card>

Character-level tokenization of the Tiny Shakespeare dataset, ideal for learning and quick experimentation.

**Statistics:**
- **Source**: Tiny Shakespeare corpus (works of Shakespeare)
- **Size**: 1.1MB raw text
- **Vocabulary**: 65 unique characters
- **Training tokens**: 1,003,854
- **Validation tokens**: 111,540
- **Training time**: ~3 minutes on A100 GPU

**Preparation:**
```bash
python data/shakespeare_char/prepare.py
```

**Outputs:**
- `train.bin` - Training set binary file
- `val.bin` - Validation set binary file
- `meta.pkl` - Character-to-int mappings

**Use case:** Learning GPT architecture, fast iteration, character-level modeling

<Info>
The character-level approach means each character in the text becomes a single token. This results in longer sequences but a tiny vocabulary.
</Info>

### Shakespeare (BPE)

**Dataset:** `shakespeare`

Byte-pair encoding (BPE) tokenization of the same Shakespeare text, compatible with GPT-2 pretrained models.

**Statistics:**
- **Source**: Same Tiny Shakespeare corpus
- **Size**: 1.1MB raw text
- **Vocabulary**: 50,257 tokens (GPT-2 BPE)
- **Training tokens**: 301,966
- **Validation tokens**: 36,059
- **Compression**: ~3x fewer tokens than character-level

**Preparation:**
```bash
python data/shakespeare/prepare.py
```

**Outputs:**
- `train.bin` - Training set binary file
- `val.bin` - Validation set binary file

**Use case:** Finetuning GPT-2 pretrained models, efficient subword tokenization

<Note>
This dataset uses the same tokenizer as GPT-2, making it compatible for finetuning any GPT-2 checkpoint (124M, 355M, 774M, or 1558M parameters).
</Note>

**Finetuning example:**
```bash
# Prepare data
python data/shakespeare/prepare.py

# Finetune GPT-2
python train.py config/finetune_shakespeare.py
```

### OpenWebText

**Dataset:** `openwebtext`

<Warning>
Requires ~54GB for HuggingFace cache plus ~17GB for the output training file. Ensure you have at least 75GB of free disk space.
</Warning>

Large-scale web text corpus for training GPT-2-scale models from scratch or continued pretraining.

**Statistics:**
- **Source**: [OpenWebText](https://huggingface.co/datasets/openwebtext) (HuggingFace)
- **Documents**: 8,013,769
- **Vocabulary**: 50,257 tokens (GPT-2 BPE)
- **Training tokens**: 9,035,582,198 (~9 billion)
- **Validation tokens**: 4,434,897 (~4 million)
- **File sizes**: train.bin ~17GB, val.bin ~8.5MB
- **Training time**: ~4 days on 8x A100 40GB GPUs

**Preparation:**
```bash
python data/openwebtext/prepare.py
```

<Info>
Preparation takes significant time as it downloads 8M documents, tokenizes them in parallel, and writes ~17GB of binary data. Consider running overnight.
</Info>

**Process:**
1. Downloads OpenWebText dataset from HuggingFace (cached in `~/.cache/huggingface`)
2. Creates 99.95% train / 0.05% validation split with shuffling
3. Tokenizes all documents using GPT-2 BPE (parallelized)
4. Appends end-of-text token (50256) after each document
5. Concatenates all tokens into single binary files

**Use case:** Training GPT-2-sized models from scratch, reproducing GPT-2 results

**Configuration:**

You can adjust parallelization in the script:

```python
# Number of workers for tokenization
num_proc = 8  # Adjust based on CPU cores

# Number of workers for dataset loading
num_proc_load_dataset = 8  # Adjust based on network speed
```

## Dataset Comparison

| Feature | shakespeare_char | shakespeare | openwebtext |
|---------|-----------------|-------------|-------------|
| **Tokenization** | Character-level | GPT-2 BPE | GPT-2 BPE |
| **Vocabulary** | 65 | 50,257 | 50,257 |
| **Training Tokens** | 1.0M | 0.3M | 9.0B |
| **Disk Space** | ~2MB | ~0.6MB | ~17GB |
| **Prep Time** | Seconds | Seconds | Hours |
| **Training Time** | Minutes | Minutes | Days |
| **Use Case** | Learning | Finetuning | Pretraining |
| **GPU Requirement** | Optional | Optional | 8x A100+ |

## Adding Custom Datasets

You can prepare your own dataset by following the existing patterns. Here's a step-by-step guide:

<Steps>
  <Step title="Create dataset directory">
    Create a new directory in `data/` for your dataset:
    
    ```bash
    mkdir data/my_dataset
    ```
  </Step>
  
  <Step title="Write prepare.py script">
    Create a `prepare.py` script based on an existing example. Choose a template:
    
    <Tabs>
      <Tab title="Character-Level">
        Use `data/shakespeare_char/prepare.py` as template:
        
        ```python
        import os
        import pickle
        import numpy as np
        
        # Load your raw text data
        input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')
        with open(input_file_path, 'r') as f:
            data = f.read()
        
        # Create character vocabulary
        chars = sorted(list(set(data)))
        vocab_size = len(chars)
        stoi = {ch: i for i, ch in enumerate(chars)}
        itos = {i: ch for i, ch in enumerate(chars)}
        
        # Encode function
        def encode(s):
            return [stoi[c] for c in s]
        
        # Train/val split
        n = len(data)
        train_data = data[:int(n * 0.9)]
        val_data = data[int(n * 0.9):]
        
        # Encode to integers
        train_ids = np.array(encode(train_data), dtype=np.uint16)
        val_ids = np.array(encode(val_data), dtype=np.uint16)
        
        # Save binary files
        train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))
        val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))
        
        # Save metadata
        meta = {
            'vocab_size': vocab_size,
            'itos': itos,
            'stoi': stoi,
        }
        with open(os.path.join(os.path.dirname(__file__), 'meta.pkl'), 'wb') as f:
            pickle.dump(meta, f)
        
        print(f"Train tokens: {len(train_ids):,}")
        print(f"Val tokens: {len(val_ids):,}")
        print(f"Vocabulary size: {vocab_size}")
        ```
      </Tab>
      
      <Tab title="BPE (Simple)">
        Use `data/shakespeare/prepare.py` as template:
        
        ```python
        import os
        import tiktoken
        import numpy as np
        
        # Load your raw text data
        input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')
        with open(input_file_path, 'r', encoding='utf-8') as f:
            data = f.read()
        
        # Train/val split
        n = len(data)
        train_data = data[:int(n * 0.9)]
        val_data = data[int(n * 0.9):]
        
        # Tokenize with GPT-2 BPE
        enc = tiktoken.get_encoding("gpt2")
        train_ids = enc.encode_ordinary(train_data)
        val_ids = enc.encode_ordinary(val_data)
        
        # Convert to numpy arrays
        train_ids = np.array(train_ids, dtype=np.uint16)
        val_ids = np.array(val_ids, dtype=np.uint16)
        
        # Save binary files
        train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))
        val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))
        
        print(f"Train tokens: {len(train_ids):,}")
        print(f"Val tokens: {len(val_ids):,}")
        ```
      </Tab>
      
      <Tab title="BPE (HuggingFace)">
        Use `data/openwebtext/prepare.py` as template for large datasets:
        
        ```python
        import os
        import numpy as np
        import tiktoken
        from datasets import load_dataset
        from tqdm import tqdm
        
        num_proc = 8
        
        # Load your dataset from HuggingFace
        dataset = load_dataset("your-username/your-dataset", num_proc=num_proc)
        
        # Create train/val split if needed
        split_dataset = dataset["train"].train_test_split(
            test_size=0.0005, seed=2357, shuffle=True
        )
        split_dataset['val'] = split_dataset.pop('test')
        
        # Tokenize with GPT-2 BPE
        enc = tiktoken.get_encoding("gpt2")
        
        def process(example):
            ids = enc.encode_ordinary(example['text'])
            ids.append(enc.eot_token)  # Add end-of-text token
            return {'ids': ids, 'len': len(ids)}
        
        # Tokenize in parallel
        tokenized = split_dataset.map(
            process,
            remove_columns=['text'],
            desc="Tokenizing",
            num_proc=num_proc,
        )
        
        # Write to binary files
        for split, dset in tokenized.items():
            arr_len = np.sum(dset['len'], dtype=np.uint64)
            filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')
            arr = np.memmap(filename, dtype=np.uint16, mode='w+', shape=(arr_len,))
            
            total_batches = 1024
            idx = 0
            for batch_idx in tqdm(range(total_batches), desc=f'Writing {filename}'):
                batch = dset.shard(
                    num_shards=total_batches, 
                    index=batch_idx, 
                    contiguous=True
                ).with_format('numpy')
                arr_batch = np.concatenate(batch['ids'])
                arr[idx : idx + len(arr_batch)] = arr_batch
                idx += len(arr_batch)
            
            arr.flush()
            print(f"{split}.bin: {arr_len:,} tokens")
        ```
      </Tab>
    </Tabs>
  </Step>
  
  <Step title="Prepare your data">
    Run the preparation script:
    
    ```bash
    python data/my_dataset/prepare.py
    ```
    
    This should create:
    - `train.bin` - Binary training data
    - `val.bin` - Binary validation data
    - `meta.pkl` - (Optional) Metadata for character-level tokenization
  </Step>
  
  <Step title="Train on your dataset">
    Specify your dataset when training:
    
    ```bash
    python train.py --dataset=my_dataset
    ```
    
    Or create a config file in `config/train_my_dataset.py`:
    
    ```python
    # Dataset
    dataset = 'my_dataset'
    
    # Model size (adjust based on dataset)
    n_layer = 6
    n_head = 6
    n_embd = 384
    
    # Training
    batch_size = 64
    block_size = 256
    max_iters = 5000
    
    # Learning rate
    learning_rate = 1e-3
    lr_decay_iters = 5000
    ```
    
    Then run:
    
    ```bash
    python train.py config/train_my_dataset.py
    ```
  </Step>
</Steps>

## Custom Tokenization

You can use alternative tokenizers beyond character-level and GPT-2 BPE:

### SentencePiece

```python
import sentencepiece as spm
import numpy as np

# Train SentencePiece model
spm.SentencePieceTrainer.train(
    input='input.txt',
    model_prefix='tokenizer',
    vocab_size=8000
)

# Load and use
sp = spm.SentencePieceProcessor(model_file='tokenizer.model')
train_ids = sp.encode(train_data, out_type=int)
train_ids = np.array(train_ids, dtype=np.uint16)
```

### Custom Vocabulary

```python
import json
import numpy as np

# Load custom vocabulary
with open('vocab.json', 'r') as f:
    vocab = json.load(f)

# Create token mappings
token_to_id = {token: i for i, token in enumerate(vocab)}
id_to_token = {i: token for i, token in enumerate(vocab)}

# Tokenize (implement your tokenization logic)
def tokenize(text):
    # Your tokenization logic here
    tokens = text.split()  # Simple example
    return [token_to_id[t] for t in tokens if t in token_to_id]

train_ids = np.array(tokenize(train_data), dtype=np.uint16)
```

<Note>
If your vocabulary size exceeds 65,535 tokens, use `dtype=np.uint32` instead of `np.uint16`. You'll also need to update the data loading code in `train.py`.
</Note>

## Best Practices

### Train/Validation Split

- **Small datasets** (&lt;10MB): Use 90/10 or 80/20 split
- **Medium datasets** (10MB-1GB): Use 95/5 or 99/1 split
- **Large datasets** (&gt;1GB): Use 99.5/0.5 or higher

<Info>
Always shuffle large datasets before splitting to ensure representative samples in the validation set.
</Info>

### Data Cleaning

Consider preprocessing your text:

```python
import re

def clean_text(text):
    # Remove excessive whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Remove control characters (except newlines)
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # Normalize unicode
    text = text.encode('utf-8', errors='ignore').decode('utf-8')
    
    return text.strip()

data = clean_text(data)
```

### Memory Considerations

For large datasets:

1. **Use memory mapping** when writing binary files:
   ```python
   arr = np.memmap(filename, dtype=np.uint16, mode='w+', shape=(total_tokens,))
   ```

2. **Process in chunks** to avoid loading entire dataset:
   ```python
   chunk_size = 1000000
   for i in range(0, len(data), chunk_size):
       chunk = data[i:i + chunk_size]
       # Process chunk
   ```

3. **Use parallelization** for tokenization:
   ```python
   from multiprocessing import Pool
   
   with Pool(8) as p:
       token_ids = p.map(tokenize_func, data_chunks)
   ```

## Validation

After preparing your dataset, validate it:

```python
import numpy as np

# Check file sizes
train_data = np.memmap('data/my_dataset/train.bin', dtype=np.uint16, mode='r')
val_data = np.memmap('data/my_dataset/val.bin', dtype=np.uint16, mode='r')

print(f"Train tokens: {len(train_data):,}")
print(f"Val tokens: {len(val_data):,}")
print(f"Train size: {len(train_data) * 2 / 1024 / 1024:.2f} MB")

# Check token range
print(f"Min token ID: {train_data.min()}")
print(f"Max token ID: {train_data.max()}")

# Verify tokens are valid
assert train_data.max() < 65536, "Token IDs exceed uint16 range"

# Sample some tokens
print(f"Sample tokens: {train_data[:100]}")
```

## Related Resources

- [Data Preparation](/api/data-preparation) - Detailed guide on the preparation process
- [Training Configuration](/training/configuration) - Configure training for your dataset
- [tiktoken](https://github.com/openai/tiktoken) - Fast BPE tokenizer library
- [HuggingFace Datasets](https://huggingface.co/docs/datasets) - Load datasets from HuggingFace Hub