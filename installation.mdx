---
title: Installation
description: Set up nanoGPT and install all required dependencies
---

# Installation

Get nanoGPT up and running in minutes. This guide covers installation, dependencies, and system requirements.

## Prerequisites

<Note>
  **Python Version**: nanoGPT requires Python 3.8 or higher. We recommend Python 3.10+ for best compatibility with PyTorch 2.0.
</Note>

### System Requirements

#### For Training

**GPU (Recommended)**:
- NVIDIA GPU with CUDA support
- Minimum 8GB VRAM for small models
- 40GB+ VRAM for GPT-2 (124M) reproduction
- CUDA 11.7+ for PyTorch 2.0 features

**CPU (Limited)**:
- Possible for small experiments
- Significantly slower training
- Useful for testing and debugging

**Apple Silicon**:
- M1/M2/M3 Macs supported via MPS backend
- 2-3x faster than CPU training
- Use `--device=mps` flag

<Warning>
  For reproducing GPT-2 (124M) on OpenWebText, you'll need at least an 8xA100 40GB node. Smaller models can be trained on consumer GPUs.
</Warning>

## Quick Install

<Steps>
  <Step title="Clone the repository">
    First, clone the nanoGPT repository from GitHub:
    
    ```bash
    git clone https://github.com/karpathy/nanoGPT.git
    cd nanoGPT
    ```
  </Step>
  
  <Step title="Install dependencies">
    Install all required Python packages using pip:
    
    ```bash
    pip install torch numpy transformers datasets tiktoken wandb tqdm
    ```
    
    This single command installs all core dependencies needed to run nanoGPT.
  </Step>
  
  <Step title="Verify installation">
    Test your installation by checking PyTorch and CUDA:
    
    ```python
    import torch
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    ```
    
    If CUDA is available, you're ready for GPU training!
  </Step>
</Steps>

## Dependencies Explained

Here's what each dependency does and why it's needed:

### Core Dependencies

#### PyTorch

```bash
pip install torch
```

[PyTorch](https://pytorch.org) is the deep learning framework that powers nanoGPT. 

**Features used**:
- Neural network modules (`nn.Module`, `nn.Linear`, etc.)
- Automatic differentiation
- GPU acceleration via CUDA
- PyTorch 2.0's `torch.compile()` for 2x speedup
- Distributed Data Parallel (DDP) for multi-GPU training

<Tip>
  For PyTorch 2.0 features, install the latest stable version or nightly build. On macOS, use the MPS-enabled build for Apple Silicon acceleration.
</Tip>

#### NumPy

```bash
pip install numpy
```

[NumPy](https://numpy.org) is used for:
- Data preprocessing and manipulation
- Memory-mapped file operations for large datasets
- Efficient numerical operations

#### Transformers (HuggingFace)

```bash
pip install transformers
```

The `transformers` library enables:
- Loading pretrained GPT-2 checkpoints from OpenAI
- Access to GPT-2 tokenizers
- Model architecture references

**Required for**:
- Finetuning from GPT-2 checkpoints
- Using `init_from='gpt2'` or other GPT-2 variants

### Data & Tokenization

#### Datasets

```bash
pip install datasets
```

HuggingFace `datasets` library provides:
- Easy access to OpenWebText dataset
- Streaming capabilities for large datasets
- Preprocessing utilities

**Required for**:
- Downloading OpenWebText (`data/openwebtext/prepare.py`)
- GPT-2 reproduction experiments

#### tiktoken

```bash
pip install tiktoken
```

OpenAI's [tiktoken](https://github.com/openai/tiktoken) provides:
- Fast BPE (Byte Pair Encoding) tokenization
- GPT-2 compatible tokenizer
- Efficient Rust-based implementation

**Required for**:
- BPE tokenization (GPT-2 style)
- OpenWebText preparation
- Finetuning with GPT-2 tokenizer

### Optional Dependencies

#### Weights & Biases (wandb)

```bash
pip install wandb
```

[wandb](https://wandb.ai) provides experiment tracking:
- Loss curves and metrics logging
- Hyperparameter tracking
- Model versioning
- Training visualization

**Enable with**:
```bash
python train.py --wandb_log=True
```

<Note>
  wandb is completely optional. Set `wandb_log=False` in your config to disable it.
</Note>

#### tqdm

```bash
pip install tqdm
```

Progress bars for:
- Data preparation scripts
- Long-running operations
- Download progress

## Platform-Specific Setup

### Linux with NVIDIA GPU

<Steps>
  <Step title="Install CUDA Toolkit">
    Ensure CUDA 11.7+ is installed:
    
    ```bash
    nvcc --version
    ```
    
    Download from [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) if needed.
  </Step>
  
  <Step title="Install PyTorch with CUDA">
    Install PyTorch with CUDA support:
    
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cu118
    ```
    
    Replace `cu118` with your CUDA version (e.g., `cu121` for CUDA 12.1).
  </Step>
  
  <Step title="Verify GPU access">
    ```bash
    python -c "import torch; print(torch.cuda.is_available())"
    ```
    
    Should print `True` if GPU is accessible.
  </Step>
</Steps>

### macOS (Apple Silicon)

<Steps>
  <Step title="Install PyTorch with MPS support">
    Install PyTorch with Metal Performance Shaders:
    
    ```bash
    pip install torch torchvision torchaudio
    ```
  </Step>
  
  <Step title="Use MPS device">
    When training, specify the MPS device:
    
    ```bash
    python train.py --device=mps
    ```
    
    This uses the on-chip GPU and can provide 2-3x speedup over CPU.
  </Step>
</Steps>

<Tip>
  For Apple Silicon Macs, make sure to get a recent PyTorch version (1.12+) that includes MPS support. See [Issue #28](https://github.com/karpathy/nanoGPT/issues/28) for more details.
</Tip>

### Windows

<Warning>
  PyTorch 2.0's `torch.compile()` is not yet fully supported on Windows. You'll need to disable it with `--compile=False` when training.
</Warning>

<Steps>
  <Step title="Install PyTorch">
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cu118
    ```
  </Step>
  
  <Step title="Disable torch.compile">
    Always add `--compile=False` to your training commands:
    
    ```bash
    python train.py --compile=False
    ```
  </Step>
</Steps>

### CPU-Only Setup

For machines without GPU:

```bash
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

When training, use:
```bash
python train.py --device=cpu --compile=False
```

## Virtual Environment (Recommended)

We strongly recommend using a virtual environment:

### Using venv

```bash
python -m venv nanogpt-env
source nanogpt-env/bin/activate  # On Windows: nanogpt-env\Scripts\activate
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

### Using conda

```bash
conda create -n nanogpt python=3.10
conda activate nanogpt
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

## Troubleshooting

### Common Issues

**Import errors**: Make sure all dependencies are installed in your active environment.

```bash
pip list | grep torch
```

**CUDA out of memory**: Reduce `batch_size` or `block_size` in your config file.

**torch.compile errors**: Add `--compile=False` to disable PyTorch 2.0 compilation.

**Slow training on Mac**: Make sure you're using `--device=mps` on Apple Silicon.

### Version Conflicts

If you encounter version conflicts:

```bash
pip install --upgrade torch numpy transformers datasets tiktoken
```

## Next Steps

Now that you have nanoGPT installed, you're ready to train your first model!

<CardGroup cols={2}>
  <Card title="Quick Start" icon="play" href="/quickstart">
    Train a character-level GPT on Shakespeare in under 5 minutes.
  </Card>
  
  <Card title="Training Guide" icon="book" href="/training/overview">
    Learn about training configurations and best practices.
  </Card>
</CardGroup>